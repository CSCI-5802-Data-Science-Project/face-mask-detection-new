{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "nbpresent": {
     "id": "b92b03cf-f8bc-435f-80d9-cdf5f0958602"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "7719f028-d416-436d-b45b-53891d1c0e15"
    }
   },
   "outputs": [],
   "source": [
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "\n",
    "def generate_label(obj):\n",
    "    if obj.find('name').text == \"with_mask\": \n",
    "        return 2 \n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\": \n",
    "        return 3 \n",
    "    return 1\n",
    "\n",
    "def generate_target(image_id, file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([image_id])\n",
    "        # Annotation is in dictionary format\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "        \n",
    "        return target\n",
    "    \n",
    "def get_filename(time: int, util_name:str =\"\"):   \n",
    "    filename = str(time.strftime('%b-%d-%Y_%H-%M-%S'))\n",
    "    if util_name != \"\":\n",
    "        filename = util_name+\"_\"+filename\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "c3b40066-d2ba-492d-b6e0-89676e4d39e4"
    }
   },
   "outputs": [],
   "source": [
    "imgs = list(sorted(os.listdir(\"dataset/images\")))\n",
    "labels = list(sorted(os.listdir(\"dataset/annotations/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "b0771d1b-1891-404d-a7fa-725e797daba0"
    }
   },
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(\"dataset/images/\")))\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = 'maksssksksss'+ str(idx) + '.png'\n",
    "        file_label = 'maksssksksss'+ str(idx) + '.xml'\n",
    "        img_path = os.path.join(\"dataset/images/\", file_image)\n",
    "        label_path = os.path.join(\"dataset/annotations/\", file_label)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        #Generate Label\n",
    "        target = generate_target(idx, label_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbpresent": {
     "id": "54b7da48-2ad0-4ebf-ab7d-064b6fb934eb"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "\n",
    "dataset = MaskDataset(data_transform)\n",
    "samples = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    samples.append(dataset[i])\n",
    "\n",
    "train_size_ratio = 0.8\n",
    "val_size_ratio = 0.1\n",
    "test_size_ratio = 0.8\n",
    "    \n",
    "# REMOVE/COMMENT THE BELOW STATEMENT\n",
    "# samples = samples[:10]\n",
    "    \n",
    "train_size = int(train_size_ratio*len(samples))\n",
    "val_size = int(val_size_ratio*len(samples))\n",
    "samples_train = samples[:train_size]\n",
    "samples_val = samples[train_size:train_size+val_size]\n",
    "samples_test = samples[train_size+val_size:]\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    " samples_train, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    " samples_val, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    " samples_test, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data_loader))\n",
    "print(len(val_data_loader))\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imgs, annotations in val_data_loader:\n",
    "#     for i in range(len(annotations)):\n",
    "#         if annotations[i]['image_id'] == 767:\n",
    "#             print(annotations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4f57731d-c3f2-4c72-a3b0-68df810a5e57"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "a50d2f75-419a-449d-8597-6e98470bb154"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_instance_segmentation(3)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "10b17f58-8ef1-48af-ad62-0a9cc01030ac"
    }
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "3a0160ce-c1bb-4278-81db-c55e988b1c44"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "train: {'loss_classifier': tensor(1.6360, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.1055, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0678, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "train: {'loss_classifier': tensor(0.8385, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.1264, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "train: {'loss_classifier': tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.1479, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0115, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "train: {'loss_classifier': tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.1584, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0100, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "train: {'loss_classifier': tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0832, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0039, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Avg training loss: 0.910215\n",
      "Avg validation loss: 0.73641247\n",
      "===================================\n",
      "Best model details:\n",
      "Epoch: 0\n",
      "Min validation loss: 0.73641247\n"
     ]
    }
   ],
   "source": [
    "time = datetime.datetime.now()\n",
    "saves_dir = os.path.join('saves', get_filename(time))\n",
    "Path(saves_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summ_filename = os.path.join(saves_dir,'tensorboard_summary')\n",
    "writer = SummaryWriter(summ_filename)\n",
    "\n",
    "num_epochs = 1\n",
    "model.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_train_dataloader = len(train_data_loader)\n",
    "\n",
    "best_val_loss = sys.maxsize\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    i = 0    \n",
    "    epoch_loss = []\n",
    "    val_epoch_loss = []\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_iou_vals = []\n",
    "    \n",
    "    for imgs, annotations in train_data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        print(\"train:\", loss_dict)\n",
    "        \n",
    "        # loss_dict consists of multiple losses such as classification loss, bounding box loss, and two other losses.\n",
    "        losses = sum([loss for loss in loss_dict.values()])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()         \n",
    "#         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "\n",
    "        epoch_loss.append(losses.detach().cpu().numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, annotations in val_data_loader:\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "            loss_dict = model(imgs, annotations)\n",
    "            losses = sum([loss for loss in loss_dict.values()])\n",
    "\n",
    "            val_epoch_loss.append(losses.detach().cpu().numpy())\n",
    "    \n",
    "    torch.save(model.state_dict(),os.path.join(saves_dir, 'model_'+str(epoch)+'_epochs.pt'))\n",
    "    avg_epoch_loss = np.mean(epoch_loss)\n",
    "    avg_val_epoch_loss = np.mean(val_epoch_loss)\n",
    "    writer.add_scalars(\"losses\", {\n",
    "        \"avg_training_loss\": avg_epoch_loss,\n",
    "        \"avg_val_loss\": avg_val_epoch_loss\n",
    "    }, epoch)\n",
    "#     writer.add_scalar(\"avg_val_loss\", avg_val_epoch_loss, epoch)\n",
    "    \n",
    "    print(\"Avg training loss:\", avg_epoch_loss)\n",
    "    print(\"Avg validation loss:\", avg_val_epoch_loss)\n",
    "    \n",
    "    \n",
    "    if avg_val_epoch_loss<best_val_loss:\n",
    "        best_val_loss=avg_val_epoch_loss\n",
    "        best_epoch = epoch\n",
    "        \n",
    "print(\"===================================\")        \n",
    "print(\"Best model details:\")\n",
    "print(\"Epoch:\", best_epoch)\n",
    "print(\"Min validation loss:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb1442cf-eeb8-450d-a9a5-0735b2733264"
    }
   },
   "source": [
    "# Function to plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "d7eca113-ce17-457c-ac8e-ed1a16383a0c"
    }
   },
   "outputs": [],
   "source": [
    "def plot_image_old(img_tensor, annotation):\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    img = img_tensor.cpu().data\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    \n",
    "    for box in annotation[\"boxes\"]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_image_new(img_tensor, annotation, block=True):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    img = img_tensor.cpu().data\n",
    "    # Display the image\n",
    "    ax.imshow( np.array( img.permute(1, 2, 0) ) )\n",
    "\n",
    "    for box, label in zip( annotation[\"boxes\"], annotation[\"labels\"] ):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        # Create a Rectangle patch\n",
    "        if label==1:\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none')\n",
    "        else:\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        ax.axis(\"off\")\n",
    "    plt.show(block=block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"sample_images/3.jpg\"\n",
    "# img_sample_input = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# if data_transform is not None:\n",
    "#     img_sample_input = data_transform(img_sample_input)\n",
    "    \n",
    "# img_sample_input = torch.unsqueeze(img_sample_input, 0).to(device)\n",
    "# # print(img_sample_input.shape)\n",
    "\n",
    "# model2 = get_model_instance_segmentation(3)\n",
    "# model2.load_state_dict(torch.load('saves/Dec-01-2020_04-47-26/model_9_epochs.pt'))\n",
    "# model2.eval()\n",
    "# model2.to(device)\n",
    "\n",
    "\n",
    "# pred_sample_input = model2(img_sample_input)\n",
    "\n",
    "# print(pred_sample_input)\n",
    "# print(\"Predict sample image with loaded model\")\n",
    "# plot_image_new(img_sample_input[0], pred_sample_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x, y) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "    bb1 = [val.detach().cpu().numpy() for val in bb1]\n",
    "    bb2 = [val.detach().cpu().numpy() for val in bb2]\n",
    "    assert bb1[1] < bb1[3]\n",
    "    assert bb2[0] < bb2[2]\n",
    "    assert bb2[1] < bb2[3]\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "    all_pred_indices= range(len(pred_boxes))\n",
    "    all_gt_indices=range(len(gt_boxes))\n",
    "    if len(all_pred_indices)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    if len(all_gt_indices)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    \n",
    "    gt_idx_thr=[]\n",
    "    pred_idx_thr=[]\n",
    "    ious=[]\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou= get_iou(gt_box, pred_box)\n",
    "            \n",
    "            if iou >iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "    iou_sort = np.argsort(ious)[::1]\n",
    "    if len(iou_sort)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    else:\n",
    "        gt_match_idx=[]\n",
    "        pred_match_idx=[]\n",
    "        for idx in iou_sort:\n",
    "            gt_idx=gt_idx_thr[idx]\n",
    "            pr_idx= pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if(gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp= len(gt_match_idx)\n",
    "        fp= len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "    return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def  get_avg_precision_at_iou(gt_boxes, pred_bb, iou_thr=0.5):\n",
    "    \n",
    "#     model_scores = get_model_scores(pred_bb)\n",
    "#     sorted_model_scores= sorted(model_scores.keys())\n",
    "# # Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "#     for img_id in pred_bb.keys():\n",
    "        \n",
    "#         arg_sort = np.argsort(pred_bb[img_id]['scores'])\n",
    "#         pred_bb[img_id]['scores'] = np.array(pred_bb[img_id]['scores'])[arg_sort].tolist()\n",
    "#         pred_bb[img_id]['boxes'] = np.array(pred_bb[img_id]['boxes'])[arg_sort].tolist()\n",
    "#     pred_boxes_pruned = deepcopy(pred_bb)\n",
    "    \n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     model_thrs = []\n",
    "#     img_results = {}\n",
    "#     # Loop over model score thresholds and calculate precision, recall\n",
    "#     for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "#             # On first iteration, define img_results for the first time:\n",
    "#         print(\"Mode score : \", model_score_thr)\n",
    "#         img_ids = gt_boxes.keys() if ithr == 0 else model_scores[model_score_thr]\n",
    "#         for img_id in img_ids:\n",
    "               \n",
    "#             gt_boxes_img = gt_boxes[img_id]\n",
    "#             box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "#             start_idx = 0\n",
    "#             for score in box_scores:\n",
    "#                 if score <= model_score_thr:\n",
    "#                     pred_boxes_pruned[img_id]\n",
    "#                     start_idx += 1\n",
    "#                 else:\n",
    "#                     break \n",
    "#             # Remove boxes, scores of lower than threshold scores:\n",
    "#             pred_boxes_pruned[img_id]['scores']= pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "#             pred_boxes_pruned[img_id]['boxes']= pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "# # Recalculate image results for this image\n",
    "#             print(img_id)\n",
    "#             img_results[img_id] = get_single_image_results(gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr=0.5)\n",
    "# # calculate precision and recall\n",
    "#         prec, rec = calc_precision_recall(img_results)\n",
    "#         precisions.append(prec)\n",
    "#         recalls.append(rec)\n",
    "#         model_thrs.append(model_score_thr)\n",
    "#     precisions = np.array(precisions)\n",
    "#     recalls = np.array(recalls)\n",
    "#     prec_at_rec = []\n",
    "#     for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "#         try:\n",
    "#             args= np.argwhere(recalls>recall_level).flatten()\n",
    "#             prec= max(precisions[args])\n",
    "#             print(recalls,\"Recall\")\n",
    "#             print(      recall_level,\"Recall Level\")\n",
    "#             print(       args, \"Args\")\n",
    "#             print(       prec, \"precision\")\n",
    "#         except ValueError:\n",
    "#             prec=0.0\n",
    "#         prec_at_rec.append(prec)\n",
    "#     avg_prec = np.mean(prec_at_rec) \n",
    "#     return {\n",
    "#         'avg_prec': avg_prec,\n",
    "#         'precisions': precisions,\n",
    "#         'recalls': recalls,\n",
    "#         'model_thrs': model_thrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_positive': 360, 'false_positive': 836, 'false_negative': 11}\n",
      "Precision: 0.3010033444816054\n"
     ]
    }
   ],
   "source": [
    "model4 = get_model_instance_segmentation(3)\n",
    "model4.load_state_dict(torch.load('saves/final_run/model_2_epochs.pt', map_location=device))\n",
    "model4.eval()\n",
    "model4.to(device)\n",
    "\n",
    "iou_vals = []\n",
    "\n",
    "accuracy_nos = {\n",
    "    'true_positive':0,\n",
    "    'false_positive':0,\n",
    "    'false_negative':0\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, annotations in val_data_loader:\n",
    "\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        pred = model4(imgs)\n",
    "        \n",
    "        for i in range(len(annotations)):\n",
    "            temp_dict = get_single_image_results(annotations[i]['boxes'], pred[i]['boxes'], 0.5)\n",
    "            \n",
    "            for k,v in temp_dict.items():\n",
    "                accuracy_nos[k] += v\n",
    "\n",
    "print(accuracy_nos)\n",
    "print('Precision:', accuracy_nos['true_positive']/(accuracy_nos['true_positive']+accuracy_nos['false_positive']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
